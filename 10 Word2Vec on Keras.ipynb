{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Word2Vec Implemented on Keras\n",
    "keras is a quite high-level deep learning library. In this notebook, we are going to implement two word2vec models: CBoW and Skip-gram. The utilized corpus is IMDB movie review dataset. http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. How to load pre-trained word vectors\n",
    "2. Reading in the IMDB Sentiment Dataset and Iterating over files in Python\n",
    "3. Build Skip-gram Model\n",
    "4. Build CBoW Model\n",
    "5. Memory-friendly Data Generation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load pre-trained word vectors\n",
    "\n",
    "- You can find the word2vec project here: https://code.google.com/archive/p/word2vec/\n",
    "- Download the word embeddings from the section **Pre-trained word and phrase vectors**. It is named `GoogleNews-vectors-negative300.bin.gz (3.4G)`\n",
    "- Use gensim that you can easily load these wordvectors and utilize their functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.05126953 -0.02233887 -0.17285156  0.16113281 -0.08447266  0.05737305\n",
      "  0.05859375 -0.08251953 -0.01538086 -0.06347656]\n",
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431607246399), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n",
      "cereal\n",
      "0.76640123\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "dog = model['dog']\n",
    "print(dog.shape)\n",
    "print(dog[:10])\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read in the IMDB Sentiment Dataset\n",
    "\n",
    "- You can access the imdb data folder in BT5153_data folder.\n",
    "- Each movie review is a text file and they are under two different folders: pos and neg.\n",
    "- We need to iterate over these files and load them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_dataset(imdb_path):\n",
    "    # imdb_path is the base path \n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    # contain two sub-folders named pos and neg\n",
    "    for cat in ['pos', 'neg']:\n",
    "        dset_path = os.path.join(imdb_path, cat)\n",
    "        # loop in each folder and get the file name for each txt.\n",
    "        for fname in sorted(os.listdir(dset_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dset_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read()) # load the data into memory\n",
    "                label = 0 if cat == 'neg' else 1\n",
    "                train_labels.append(label)\n",
    "    imdbdf = pd.DataFrame(\n",
    "             {'text': train_texts,\n",
    "              'label': train_labels}\n",
    "             )\n",
    "    # shuffle the whole dataset\n",
    "    imdbdf = imdbdf.sample(frac=1).reset_index(drop=True)\n",
    "    # Return the dataset in dataframe format\n",
    "    return imdbdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples shape : 25000\n"
     ]
    }
   ],
   "source": [
    "df_corpus = load_imdb_dataset('./imdb/imdb')\n",
    "print ('Train samples shape :', df_corpus.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  No likeable characters (the lead is a combinat...      0\n",
      "1  Man, I really wanted to like these shows. I am...      0\n",
      "2  His choice of films, the basic 'conceit' of th...      1\n",
      "3  The story of the untouchable who acted like a ...      1\n",
      "4  It really impresses me that it got made. The d...      0\n"
     ]
    }
   ],
   "source": [
    "# 1 denotes positive and 0 is negative\n",
    "print(df_corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "def clean_txt(raw_txt):\n",
    "    # Function to clean raw text\n",
    "    # 1. Remove HTML\n",
    "    raw_txt = BeautifulSoup(raw_txt, \"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_txt) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                                             \n",
    "    # \n",
    "    #\n",
    "    # 4. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus['text'] = df_corpus.text.apply(clean_txt)\n",
    "corpus = df_corpus.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# check the corpus type, which is a list of string\n",
    "print(type(corpus))\n",
    "print(type(corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tokenization from Keras\n",
    "\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# learn the vocab\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "man i really wanted to like these shows i am starving for some good television and i applaud tnt for providing these opportunites but sadly i am in the minority i guess when it comes to the cinematic stephen king as brilliant as king s writing is the irony is that it simply doesn t translate well to the screen big or small with few exceptions very few the king experience cannot be filmed with the same impact that the stories have when read many people would disagree with this but i m sure that in their heart of hearts they have to admit that the best filmed king story is but a pale memory of the one they read the reason is simple the average king story takes place in the mind scape of the characters in the story he gives us glimpses of their inner thoughts their emotions and their sometimes fractured or unreal points of view in short king takes the reader places where you can t put a panavision camera as an audience watching the filmed king we re left with less than half the information than the reader has access to it s not too far a stretch to claim that one becomes a character in a king story they read whereas one is limited to petty voyeurism of that same character when filmed for as long as king writes hollywood will try shooting everything that comes out of his word processor without any regard to whether or not they should i don t blame the filmmakers for trying but it takes an incredible amount of talent and circumspection to pull off the elusive stephen king adaptation that works the task is akin to turning lead into gold or some arcane zen mastery oh well better luck next time\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus[1]))\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `fit_on_texts` function is trying to build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124, 9, 64, 463, 5, 38, 131, 285, 9, 238, 9697, 15, 48, 49, 678, 2, 9, 6051, 17938, 15, 3707, 131, 46354, 18, 1012, 9, 238, 8, 1, 5785, 9, 476, 53, 7, 261, 5, 1, 1337, 1630, 601, 14, 518, 14, 601, 12, 478, 6, 1, 3117, 6, 11, 7, 327, 151, 20, 7067, 71, 5, 1, 260, 191, 41, 384, 16, 169, 5563, 54, 169, 1, 601, 572, 553, 28, 796, 16, 1, 170, 1455, 11, 1, 525, 27, 53, 328, 108, 77, 60, 3407, 16, 10, 18, 9, 140, 248, 11, 8, 66, 468, 4, 3339, 32, 27, 5, 955, 11, 1, 116, 796, 601, 62, 6, 18, 3, 6279, 1722, 4, 1, 29, 32, 328, 1, 282, 6, 593, 1, 832, 601, 62, 301, 268, 8, 1, 326, 24100, 4, 1, 102, 8, 1, 62, 24, 402, 177, 7160, 4, 66, 2359, 2289, 66, 1416, 2, 66, 507, 15097, 41, 4865, 743, 4, 633, 8, 342, 601, 301, 1, 5050, 1339, 117, 21, 50, 20, 271, 3, 21016, 362, 14, 34, 299, 147, 1, 796, 601, 68, 149, 312, 16, 324, 72, 316, 1, 1602, 72, 1, 5050, 46, 4535, 5, 7, 12, 23, 97, 226, 3, 3174, 5, 2267, 11, 29, 452, 3, 103, 8, 3, 601, 62, 32, 328, 3100, 29, 6, 1737, 5, 4914, 14041, 4, 11, 170, 103, 53, 796, 15, 14, 193, 14, 601, 4456, 333, 78, 347, 1183, 283, 11, 261, 44, 4, 26, 664, 24101, 205, 99, 2859, 5, 710, 41, 23, 32, 142, 9, 88, 20, 1801, 1, 1024, 15, 263, 18, 7, 301, 34, 1029, 1142, 4, 657, 2, 46355, 5, 1572, 122, 1, 10872, 1630, 601, 1234, 11, 487, 1, 2763, 6, 5971, 5, 1567, 475, 82, 1781, 41, 48, 15686, 12766, 11116, 435, 71, 127, 1970, 369, 56]\n"
     ]
    }
   ],
   "source": [
    "# from string to a sequence of intergers\n",
    "# each word will be convereted to its vocab index\n",
    "seq_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "print(seq_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we are going to use a toy corpus instead of the IMDB corpus for a quick demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n",
      "4\n",
      "1\n",
      "2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# let us check the texts_to_sequences function\n",
    "toy_corpus = ['king is a strong man', \n",
    "              'queen is a wise woman', \n",
    "              'boy is a young man',\n",
    "              'girl is a young woman',\n",
    "              'prince is a young king',\n",
    "              'princess is a young queen',\n",
    "               'man is strong', \n",
    "               'woman is pretty',\n",
    "               'prince is a boy will be king',\n",
    "               'princess is a girl will be queen']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(toy_corpus)\n",
    "toy_seq_corpus = tokenizer.texts_to_sequences(toy_corpus)\n",
    "print(toy_seq_corpus[0])\n",
    "print(tokenizer.word_index['king'])\n",
    "print(tokenizer.word_index['is'])\n",
    "print(tokenizer.word_index['a'])\n",
    "print(tokenizer.word_index['strong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1fc3db1c7863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The KeyError means that the Tokenizer reserves 0 as an OOV words.\n",
    "- In practive, the first ebmedding in word embedding martix is for unkown words or chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Skip-gram Model\n",
    "\n",
    "- Here, we only use toy corpus for demo purpose.\n",
    "- Target: predict the nearby words based on the center word.\n",
    "<img src=\"word2vec-skip-gram.png\" alt=\"cbow\"\n",
    "\ttitle=\"cbow pic\" width=\"250\" height=\"150\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For skip-gram,  training data generation**:\n",
    "\n",
    "the input x is the center word index, the output x is one hot vector of the neary word index.\n",
    "For example, the toy corpus only contain two sentences.\n",
    "```\n",
    "I like apple \n",
    "I like reading books\n",
    "```\n",
    "1. The first step: build a vocab. which can be regarded as a mapping from words to interget index.\n",
    "\n",
    "Here, OOV-> 0, I -> 1, like -> 2, apple -> 3, reading -> 4, books -> 5.\n",
    "\n",
    "2. Then, we scan the corpus and creat the pair of center word and nearby word. Here, we set the window size is `one`.\n",
    "We have the following pair of input x and target y.\n",
    "\n",
    "<pre>\n",
    "words pair              numerical input       numerical output\n",
    "\n",
    "(I, like)                       1               [0,0,1,0,0,0]\n",
    "\n",
    "(like, I)                       2               [0,1,0,0,0,0]\n",
    "\n",
    "(like, apple)                   2               [0,0,0,1,0,0]\n",
    "\n",
    "(apple, like)                   4               [0,0,1,0,0,0]\n",
    "\n",
    "(I, like)\n",
    "\n",
    "(like, I)\n",
    "\n",
    "(like, reading)              \n",
    "\n",
    "(reading, like)\n",
    "\n",
    "(reading, books)\n",
    "\n",
    "(books, reading)                 5              [0,0,0,0,1,0]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the collection of lists of words index\n",
    "    window_size is the context size that defines 'nearby' words\n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    in_words   = [] \n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            for i in range(s, e):\n",
    "                if 0<= i < L and i != index:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(to_categorical(words[i], V))\n",
    "    return (in_words, labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus one is for OOV words\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 5\n",
    "window_size = 4\n",
    "ith = 0\n",
    "input_x, target_y =  generate_data(toy_seq_corpus, window_size, V)\n",
    "input_x           = np.array(input_x,dtype=np.int32)\n",
    "target_y          = np.array(target_y,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the first pair of input and output\n",
      "[4]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "check the third pair of input and output\n",
      "[4]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('check the first pair of input and output')\n",
    "print(input_x[0])\n",
    "print(target_y[0])#the onehot vector\n",
    "print('check the third pair of input and output')\n",
    "print(input_x[2])\n",
    "print(target_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "print(toy_seq_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Config**\n",
    "\n",
    "It consists of two layers:\n",
    "\n",
    "1. The first layer is embeddings layer, which perform the lookup operation. Given the word index as the input, the layer output will return the corresponding vector\n",
    "\n",
    "2. The second layer is softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embeddings Layer**:\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "1. input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "2. output_dim: int >= 0. Dimension of the dense embedding.\n",
    "3. embeddings_initializer: Initializer for the embeddings matrix (see initializers).\n",
    "4. input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=17, output_dim=5, input_length=1, embeddings_initializer=\"glorot_uniform\")`\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=5, activation=\"softmax\", units=17)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 5)              85        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                102       \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 2.8364\n",
      "Epoch 2/30\n",
      "204/204 [==============================] - 0s 162us/step - loss: 2.8191\n",
      "Epoch 3/30\n",
      "204/204 [==============================] - 0s 221us/step - loss: 2.8032\n",
      "Epoch 4/30\n",
      "204/204 [==============================] - 0s 191us/step - loss: 2.7884\n",
      "Epoch 5/30\n",
      "204/204 [==============================] - 0s 191us/step - loss: 2.7746\n",
      "Epoch 6/30\n",
      "204/204 [==============================] - 0s 191us/step - loss: 2.7616\n",
      "Epoch 7/30\n",
      "204/204 [==============================] - 0s 201us/step - loss: 2.7489\n",
      "Epoch 8/30\n",
      "204/204 [==============================] - 0s 196us/step - loss: 2.7375\n",
      "Epoch 9/30\n",
      "204/204 [==============================] - 0s 206us/step - loss: 2.7260\n",
      "Epoch 10/30\n",
      "204/204 [==============================] - 0s 201us/step - loss: 2.7154\n",
      "Epoch 11/30\n",
      "204/204 [==============================] - 0s 186us/step - loss: 2.7051\n",
      "Epoch 12/30\n",
      "204/204 [==============================] - 0s 201us/step - loss: 2.6953\n",
      "Epoch 13/30\n",
      "204/204 [==============================] - 0s 196us/step - loss: 2.6859\n",
      "Epoch 14/30\n",
      "204/204 [==============================] - 0s 221us/step - loss: 2.6769\n",
      "Epoch 15/30\n",
      "204/204 [==============================] - 0s 191us/step - loss: 2.6686\n",
      "Epoch 16/30\n",
      "204/204 [==============================] - 0s 181us/step - loss: 2.6605\n",
      "Epoch 17/30\n",
      "204/204 [==============================] - 0s 176us/step - loss: 2.6526\n",
      "Epoch 18/30\n",
      "204/204 [==============================] - 0s 201us/step - loss: 2.6454\n",
      "Epoch 19/30\n",
      "204/204 [==============================] - 0s 343us/step - loss: 2.6384\n",
      "Epoch 20/30\n",
      "204/204 [==============================] - 0s 289us/step - loss: 2.6315\n",
      "Epoch 21/30\n",
      "204/204 [==============================] - 0s 211us/step - loss: 2.6250\n",
      "Epoch 22/30\n",
      "204/204 [==============================] - 0s 240us/step - loss: 2.6185\n",
      "Epoch 23/30\n",
      "204/204 [==============================] - 0s 221us/step - loss: 2.6124\n",
      "Epoch 24/30\n",
      "204/204 [==============================] - 0s 216us/step - loss: 2.6064\n",
      "Epoch 25/30\n",
      "204/204 [==============================] - 0s 240us/step - loss: 2.6009\n",
      "Epoch 26/30\n",
      "204/204 [==============================] - 0s 309us/step - loss: 2.5953\n",
      "Epoch 27/30\n",
      "204/204 [==============================] - 0s 245us/step - loss: 2.5900\n",
      "Epoch 28/30\n",
      "204/204 [==============================] - 0s 240us/step - loss: 2.5849\n",
      "Epoch 29/30\n",
      "204/204 [==============================] - 0s 221us/step - loss: 2.5798\n",
      "Epoch 30/30\n",
      "204/204 [==============================] - 0s 216us/step - loss: 2.5752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f36160>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.fit(input_x, target_y, batch_size=8, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How to save the learned word vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **the saved format for word vectors** the first number is the vocabulary size, 5 is dimension\n",
    "<img src=\"saved_format.jpg\" alt=\"cbow\"\n",
    "\ttitle=\"saved format\" width=\"550\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **we can use gensim**\n",
    "\n",
    "Gensim is a production-ready open-source library for NLP problems.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.9203758239746094),\n",
       " ('woman', 0.8602206707000732),\n",
       " ('princess', 0.8566994667053223),\n",
       " ('queen', 0.7101820707321167),\n",
       " ('be', 0.6660196781158447),\n",
       " ('will', 0.5517114996910095),\n",
       " ('king', 0.5441617369651794),\n",
       " ('a', 0.38888657093048096),\n",
       " ('girl', 0.3134895861148834),\n",
       " ('is', 0.2508010268211365)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build CBoW Model\n",
    "\n",
    "- CBoW's target is the prediction of center word.\n",
    "<img src=\"word2vec-cbow.png\" alt=\"cbow\"\n",
    "\ttitle=\"cbow pic\" width=\"250\" height=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For cbow,  training data generation**:\n",
    "\n",
    "the input x is the list of  context word index, the output x is one hot vector of the center word.\n",
    "For example, the toy corpus only contain two sentences.\n",
    "```\n",
    "I like apple \n",
    "I like reading books\n",
    "```\n",
    "1. The first step: build a vocab. which can be regarded as a mapping from words to interget index. \n",
    "Here OOV->0, I -> 1, like -> 2, apple -> 3, reading -> 4 books -> 5.\n",
    "\n",
    "2. Then, we scan the corpus and creat the pair of list of nearby word and center word. Here, we set the window size is `one`.\n",
    "We have the following pair of input x and target y.\n",
    "\n",
    "<pre>\n",
    "words pair                     numerical input       numerical output\n",
    "\n",
    "([like], I)                        [2]                 [0,1,0,0,0,0]\n",
    "\n",
    "([I, apple], like)                 [1,3]               [0,0,1,0,0,0]\n",
    "\n",
    "([like], apple)                    [2]                 [0,0,0,1,0,0]\n",
    "\n",
    "([like], I)                        [2]                 [0,1,0,0,0,0]\n",
    " \n",
    "([I, reading], like)               [1,4]               [0,1,0,0,0,0]\n",
    "\n",
    "([like, books], reading)           [2,5]               [0,0,0,0,1,0]\n",
    "\n",
    "([reading], books)                 [4]                 [0,0,0,0,0,1]\n",
    "</pre>\n",
    "\n",
    "3. At last, sometimes, we can not get the input context with enough length. For example, the first pair's numerical input only has one word index insetad of two. What we can do here is padding the short input so that all input data have the same length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prepare the training and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "def generate_data(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the list of sequence of words index\n",
    "    window_size is used to define  \n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    context_words   = []\n",
    "    center_words    = []\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)           \n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = to_categorical(labels, V)\n",
    "            context_words.append(x)\n",
    "            center_words.append(y)\n",
    "    return context_words, center_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ith = 0\n",
    "input_x, target_y = generate_data(toy_seq_corpus, window_size, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1, 8)\n",
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "input_x = np.array(input_x)\n",
    "print(input_x.shape)\n",
    "input_x = np.squeeze(input_x)  # sequeeze the second dimesion as on\n",
    "print(input_x.shape)\n",
    "target_y = np.array(target_y)\n",
    "target_y = np.squeeze(target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 2 8 5]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(input_x[0])\n",
    "print(target_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "print(toy_seq_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lambda Layer**:\n",
    "\n",
    "Wraps arbitrary expression as a Layer object.\n",
    "    1. function: The function to be evaluated. Takes input tensor as first argument. usually based on backend\n",
    "    2. output_shape: Expected output shape from function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "# sum all embeddings \n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "## add softmax layer\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 8, 5)              85        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 17)                102       \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.8350\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 312us/step - loss: 2.8312\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 532us/step - loss: 2.8282\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 360us/step - loss: 2.8253\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 280us/step - loss: 2.8218\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 400us/step - loss: 2.8193\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 280us/step - loss: 2.8166\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 260us/step - loss: 2.8135\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 260us/step - loss: 2.8102\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 280us/step - loss: 2.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11550898>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# Train the model, iterating on the data in batches of 512 samples\n",
    "cbow.fit(input_x, target_y, batch_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Memory-friendly Data Generation\n",
    "\n",
    "- Here, we modify the data generation function of skip-gram\n",
    "- `yield`: it will return generators. And generators do not store all the values in memory. It will return value during each iteration.\n",
    "\n",
    "sample code\n",
    "```\n",
    "generator = (x * x for x in range(3))\n",
    "for i in generator:\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_live(corpus, window_size, V):\n",
    "    \"\"\"\n",
    "    corpus is the list of sequence of words index\n",
    "    window_size is used to define  \n",
    "    V is the vocab Size\n",
    "    \"\"\"\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        labels   = []\n",
    "        in_words = [] \n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            for i in range(s, e):\n",
    "                if 0<= i < L and i != index:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "        x = np.array(in_words,dtype=np.int32)\n",
    "        y = to_categorical(labels, V)\n",
    "        yield (x, y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25.925078868865967\n",
      "1 25.895569801330566\n",
      "2 25.86624312400818\n",
      "3 25.836897134780884\n",
      "4 25.807478189468384\n",
      "5 25.777966499328613\n",
      "6 25.74836039543152\n",
      "7 25.718678951263428\n",
      "8 25.68894910812378\n",
      "9 25.659205436706543\n",
      "10 25.62948775291443\n",
      "11 25.599831104278564\n",
      "12 25.570274353027344\n",
      "13 25.540847301483154\n",
      "14 25.51158118247986\n",
      "15 25.482503175735474\n",
      "16 25.453638553619385\n",
      "17 25.425008058547974\n",
      "18 25.396633625030518\n",
      "19 25.36853051185608\n",
      "20 25.340715646743774\n",
      "21 25.31320285797119\n",
      "22 25.28600311279297\n",
      "23 25.259127378463745\n",
      "24 25.232582330703735\n",
      "25 25.206376314163208\n",
      "26 25.180516004562378\n",
      "27 25.155004024505615\n",
      "28 25.12984275817871\n",
      "29 25.105035066604614\n",
      "30 25.08058214187622\n",
      "31 25.056483268737793\n",
      "32 25.0327365398407\n",
      "33 25.00934100151062\n",
      "34 24.98629379272461\n",
      "35 24.963590621948242\n",
      "36 24.94122886657715\n",
      "37 24.919200897216797\n",
      "38 24.897504091262817\n",
      "39 24.87613296508789\n",
      "40 24.855081796646118\n",
      "41 24.83434009552002\n",
      "42 24.81390690803528\n",
      "43 24.793773651123047\n",
      "44 24.773934364318848\n",
      "45 24.75438094139099\n",
      "46 24.735108375549316\n",
      "47 24.71610975265503\n",
      "48 24.697376489639282\n",
      "49 24.678903818130493\n"
     ]
    }
   ],
   "source": [
    "## here you should define skipgram from scratch\n",
    "\n",
    "for ite in range(50):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_live(toy_seq_corpus, window_size, V):\n",
    "        #updated parameters based on data samples provided without regard to any fixed batch size\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
