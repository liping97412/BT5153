{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 RNN Language Model on Keras\n",
    "In this notebook, we are going to implement three language models following three language models following their own assumptions\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Recap on tokenizer, convert string to a list of word index\n",
    "\n",
    "2. One word to One word p(wi|wi-1)\n",
    "\n",
    "3. Fixed number of word to One word p(wi|wi-1, wi-2,wi-3)\n",
    "\n",
    "4. Variable number of input words p(wi|wi-1, wi-2, ....w1) by Stateful RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap on Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adopted Corpus \n",
    "- The following toy corpus is from Chapter 1 of Harry Potter\n",
    "-  And we are using the trained language model to write our own `Harry Potter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense\\n\n",
    "          Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "          drills. He was a big, beefy man with hardly any neck, although he did\n",
    "          have a very large mustache. Mrs. Dursley was thin and blonde and had \n",
    "          nearly twice the usual amount of neck, which came in very useful as she \n",
    "          spent so much of her time craning over garden fences, spying on the \n",
    "          neighbors. The Dursleys had a small son called Dudley and in their \n",
    "          opinion there was no finer boy anywhere. \n",
    "          The Dursleys had everything they wanted, but they also had a secret, and \n",
    "          their greatest fear was that somebody would discover it. They didn't \n",
    "          think they could bear it if anyone found out about the Potters. Mrs. \n",
    "          Potter was Mrs. Dursley's sister, but they hadn't met for several years; \n",
    "          in fact, Mrs. Dursley pretended she didn't have a sister, because her \n",
    "          sister and her good-for-nothing husband were as unDursleyish as it was \n",
    "          possible to be. The Dursleys shuddered to think what the neighbors would \n",
    "          say if the Potters arrived in the street. The Dursleys knew that the \n",
    "          Potters had a small son, too, but they had never even seen him. This boy \n",
    "          was another good reason for keeping the Potters away; they didn't want \n",
    "          Dudley mixing with a child like that\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map letters to interger values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])  ## develop mapping from words to unique integers\n",
    "encoded = tokenizer.texts_to_sequences([corpus])[0]  ## sequence of text can be convereted to sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 5, 7, 9, 10, 46, 47, 48, 49, 11]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 149\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 ## plus one is for OOV words\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['mrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-gram Language Model\n",
    "\n",
    "- One word in, one word out\n",
    "- The assumption is that the current word only depends on the previous one word.\\\n",
    "- P('I finally finish BT5153') = P(I) * P(finally/I) * P(finish/finally) * P(BT5153/finiSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data generation\n",
    "\n",
    ". From the training sentence `I finally finish BT5153`, we can generate the following training corpus:\n",
    "\n",
    "<pre>\n",
    "Input X               Target y\n",
    "-------------------------------\n",
    "I                     Finally\n",
    "\n",
    "Finally               Finish\n",
    "\n",
    "Finish                BT5153\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 262\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word index\n",
      "25\n",
      "mr\n",
      "5\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print('Input word index')\n",
    "print(X[0])\n",
    "print(tokenizer.index_word[X[0]])\n",
    "print(y[0])\n",
    "print(tokenizer.index_word[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  one hot encoding\n",
    "\n",
    "Neural networks output for multi-class classifcations can only be one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 15)             2235      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                13200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 149)               7599      \n",
      "=================================================================\n",
      "Total params: 23,034\n",
      "Trainable params: 23,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 15\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=1))  # it is one word in therefore, the input lengt is 1\n",
    "model.add(LSTM(50))                                   # the output size of LSTM layer is 50\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 1s - loss: 5.0038 - acc: 0.0382\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.9994 - acc: 0.0725\n",
      "Epoch 3/100\n",
      " - 0s - loss: 4.9956 - acc: 0.0649\n",
      "Epoch 4/100\n",
      " - 0s - loss: 4.9915 - acc: 0.0534\n",
      "Epoch 5/100\n",
      " - 0s - loss: 4.9874 - acc: 0.0534\n",
      "Epoch 6/100\n",
      " - 0s - loss: 4.9823 - acc: 0.0534\n",
      "Epoch 7/100\n",
      " - 0s - loss: 4.9767 - acc: 0.0534\n",
      "Epoch 8/100\n",
      " - 0s - loss: 4.9700 - acc: 0.0534\n",
      "Epoch 9/100\n",
      " - 0s - loss: 4.9623 - acc: 0.0649\n",
      "Epoch 10/100\n",
      " - 0s - loss: 4.9530 - acc: 0.0649\n",
      "Epoch 11/100\n",
      " - 0s - loss: 4.9411 - acc: 0.0649\n",
      "Epoch 12/100\n",
      " - 0s - loss: 4.9272 - acc: 0.0649\n",
      "Epoch 13/100\n",
      " - 0s - loss: 4.9109 - acc: 0.0649\n",
      "Epoch 14/100\n",
      " - 0s - loss: 4.8906 - acc: 0.0649\n",
      "Epoch 15/100\n",
      " - 0s - loss: 4.8662 - acc: 0.0649\n",
      "Epoch 16/100\n",
      " - 0s - loss: 4.8380 - acc: 0.0649\n",
      "Epoch 17/100\n",
      " - 0s - loss: 4.8030 - acc: 0.0649\n",
      "Epoch 18/100\n",
      " - 0s - loss: 4.7644 - acc: 0.0649\n",
      "Epoch 19/100\n",
      " - 0s - loss: 4.7156 - acc: 0.0649\n",
      "Epoch 20/100\n",
      " - 0s - loss: 4.6631 - acc: 0.0649\n",
      "Epoch 21/100\n",
      " - 0s - loss: 4.6032 - acc: 0.0649\n",
      "Epoch 22/100\n",
      " - 0s - loss: 4.5426 - acc: 0.0649\n",
      "Epoch 23/100\n",
      " - 0s - loss: 4.4788 - acc: 0.0725\n",
      "Epoch 24/100\n",
      " - 0s - loss: 4.4134 - acc: 0.0725\n",
      "Epoch 25/100\n",
      " - 0s - loss: 4.3523 - acc: 0.0725\n",
      "Epoch 26/100\n",
      " - 0s - loss: 4.2923 - acc: 0.0725\n",
      "Epoch 27/100\n",
      " - 0s - loss: 4.2354 - acc: 0.0878\n",
      "Epoch 28/100\n",
      " - 0s - loss: 4.1822 - acc: 0.0916\n",
      "Epoch 29/100\n",
      " - 0s - loss: 4.1306 - acc: 0.1031\n",
      "Epoch 30/100\n",
      " - 0s - loss: 4.0814 - acc: 0.1145\n",
      "Epoch 31/100\n",
      " - 0s - loss: 4.0331 - acc: 0.1183\n",
      "Epoch 32/100\n",
      " - 0s - loss: 3.9849 - acc: 0.1298\n",
      "Epoch 33/100\n",
      " - 0s - loss: 3.9374 - acc: 0.1489\n",
      "Epoch 34/100\n",
      " - 0s - loss: 3.8911 - acc: 0.1565\n",
      "Epoch 35/100\n",
      " - 0s - loss: 3.8451 - acc: 0.1756\n",
      "Epoch 36/100\n",
      " - 0s - loss: 3.7997 - acc: 0.1832\n",
      "Epoch 37/100\n",
      " - 0s - loss: 3.7552 - acc: 0.1985\n",
      "Epoch 38/100\n",
      " - 0s - loss: 3.7120 - acc: 0.1985\n",
      "Epoch 39/100\n",
      " - 0s - loss: 3.6687 - acc: 0.1947\n",
      "Epoch 40/100\n",
      " - 0s - loss: 3.6256 - acc: 0.2137\n",
      "Epoch 41/100\n",
      " - 0s - loss: 3.5840 - acc: 0.2176\n",
      "Epoch 42/100\n",
      " - 0s - loss: 3.5411 - acc: 0.2099\n",
      "Epoch 43/100\n",
      " - 0s - loss: 3.4990 - acc: 0.2137\n",
      "Epoch 44/100\n",
      " - 0s - loss: 3.4572 - acc: 0.2137\n",
      "Epoch 45/100\n",
      " - 0s - loss: 3.4167 - acc: 0.2176\n",
      "Epoch 46/100\n",
      " - 0s - loss: 3.3754 - acc: 0.2252\n",
      "Epoch 47/100\n",
      " - 0s - loss: 3.3352 - acc: 0.2328\n",
      "Epoch 48/100\n",
      " - 0s - loss: 3.2955 - acc: 0.2366\n",
      "Epoch 49/100\n",
      " - 0s - loss: 3.2559 - acc: 0.2405\n",
      "Epoch 50/100\n",
      " - 0s - loss: 3.2154 - acc: 0.2634\n",
      "Epoch 51/100\n",
      " - 0s - loss: 3.1749 - acc: 0.2634\n",
      "Epoch 52/100\n",
      " - 0s - loss: 3.1348 - acc: 0.2634\n",
      "Epoch 53/100\n",
      " - 0s - loss: 3.0956 - acc: 0.2748\n",
      "Epoch 54/100\n",
      " - 0s - loss: 3.0543 - acc: 0.2863\n",
      "Epoch 55/100\n",
      " - 0s - loss: 3.0150 - acc: 0.3053\n",
      "Epoch 56/100\n",
      " - 0s - loss: 2.9738 - acc: 0.3130\n",
      "Epoch 57/100\n",
      " - 0s - loss: 2.9343 - acc: 0.3168\n",
      "Epoch 58/100\n",
      " - 0s - loss: 2.8938 - acc: 0.3244\n",
      "Epoch 59/100\n",
      " - 0s - loss: 2.8546 - acc: 0.3473\n",
      "Epoch 60/100\n",
      " - 0s - loss: 2.8138 - acc: 0.3626\n",
      "Epoch 61/100\n",
      " - 0s - loss: 2.7725 - acc: 0.3702\n",
      "Epoch 62/100\n",
      " - 0s - loss: 2.7336 - acc: 0.3855\n",
      "Epoch 63/100\n",
      " - 0s - loss: 2.6933 - acc: 0.3931\n",
      "Epoch 64/100\n",
      " - 0s - loss: 2.6529 - acc: 0.3893\n",
      "Epoch 65/100\n",
      " - 0s - loss: 2.6141 - acc: 0.4008\n",
      "Epoch 66/100\n",
      " - 0s - loss: 2.5747 - acc: 0.4046\n",
      "Epoch 67/100\n",
      " - 0s - loss: 2.5357 - acc: 0.3969\n",
      "Epoch 68/100\n",
      " - 0s - loss: 2.4965 - acc: 0.4237\n",
      "Epoch 69/100\n",
      " - 0s - loss: 2.4573 - acc: 0.4275\n",
      "Epoch 70/100\n",
      " - 0s - loss: 2.4201 - acc: 0.4313\n",
      "Epoch 71/100\n",
      " - 0s - loss: 2.3806 - acc: 0.4427\n",
      "Epoch 72/100\n",
      " - 0s - loss: 2.3435 - acc: 0.4466\n",
      "Epoch 73/100\n",
      " - 0s - loss: 2.3066 - acc: 0.4504\n",
      "Epoch 74/100\n",
      " - 0s - loss: 2.2715 - acc: 0.4504\n",
      "Epoch 75/100\n",
      " - 0s - loss: 2.2354 - acc: 0.4656\n",
      "Epoch 76/100\n",
      " - 0s - loss: 2.2003 - acc: 0.4771\n",
      "Epoch 77/100\n",
      " - 0s - loss: 2.1654 - acc: 0.4809\n",
      "Epoch 78/100\n",
      " - 0s - loss: 2.1313 - acc: 0.5000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 2.0966 - acc: 0.5000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 2.0650 - acc: 0.4962\n",
      "Epoch 81/100\n",
      " - 0s - loss: 2.0332 - acc: 0.5153\n",
      "Epoch 82/100\n",
      " - 0s - loss: 2.0012 - acc: 0.5076\n",
      "Epoch 83/100\n",
      " - 0s - loss: 1.9688 - acc: 0.5191\n",
      "Epoch 84/100\n",
      " - 0s - loss: 1.9381 - acc: 0.5191\n",
      "Epoch 85/100\n",
      " - 0s - loss: 1.9063 - acc: 0.5267\n",
      "Epoch 86/100\n",
      " - 0s - loss: 1.8785 - acc: 0.5267\n",
      "Epoch 87/100\n",
      " - 0s - loss: 1.8490 - acc: 0.5305\n",
      "Epoch 88/100\n",
      " - 0s - loss: 1.8221 - acc: 0.5382\n",
      "Epoch 89/100\n",
      " - 0s - loss: 1.7941 - acc: 0.5420\n",
      "Epoch 90/100\n",
      " - 0s - loss: 1.7659 - acc: 0.5458\n",
      "Epoch 91/100\n",
      " - 0s - loss: 1.7391 - acc: 0.5420\n",
      "Epoch 92/100\n",
      " - 0s - loss: 1.7122 - acc: 0.5496\n",
      "Epoch 93/100\n",
      " - 0s - loss: 1.6862 - acc: 0.5496\n",
      "Epoch 94/100\n",
      " - 0s - loss: 1.6608 - acc: 0.5649\n",
      "Epoch 95/100\n",
      " - 0s - loss: 1.6354 - acc: 0.5725\n",
      "Epoch 96/100\n",
      " - 0s - loss: 1.6134 - acc: 0.5763\n",
      "Epoch 97/100\n",
      " - 0s - loss: 1.5901 - acc: 0.5878\n",
      "Epoch 98/100\n",
      " - 0s - loss: 1.5676 - acc: 0.5840\n",
      "Epoch 99/100\n",
      " - 0s - loss: 1.5452 - acc: 0.5878\n",
      "Epoch 100/100\n",
      " - 0s - loss: 1.5248 - acc: 0.5916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2eccfca00f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### call model to generate text\n",
    "\n",
    "- Here, we can use our trained model to write our own `Harry Potter`\n",
    "- The idea is that we can iteratively call the language model to select the word with the highest prob scores. \n",
    "    - Inital the first word as w0\n",
    "    - Loop index i from 0 to the pre-defined length n_words\n",
    "        1. feed the word wi into the model\n",
    "        2. assign the word with the highest probs. score to wi+1\n",
    "        3. index i = i + 1\n",
    "    - At last, given word w0, we have the complete sentence w0, w1,..., wn_words\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq_one_word(model, tokenizer, seed_text, n_words):\n",
    "    \"\"\"\n",
    "    Model inputs:\n",
    "    1 model: language model\n",
    "    2 tokenizer: it maintain the same mapping (word to index) as the model used\n",
    "    3 seed_text: the inital input word string\n",
    "    4 n_words: the length of target sentence\n",
    "    \"\"\"\n",
    "    in_text, result = seed_text, seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = np.array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write our own `Harry Potter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dudley and mrs dursley was mrs dursley\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq_one_word(model, tokenizer, 'dudley', 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram Language Model\n",
    "\n",
    "- Fixed Number word in, one word out\n",
    "- The assumption is that the current word only depends on the previous N word.\\\n",
    "- If N=2, P('I finally finish BT5153') = P(I) * P(finally/I) * P(finish/I, finally) * P(BT5153/finally,finish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data generation\n",
    "\n",
    ". From the training sentence `I finally finish BT5153`, we can generate the following training corpus with N=2:\n",
    "\n",
    "<pre>\n",
    "Input X               Target y\n",
    "-------------------------------\n",
    "I                     Finally\n",
    "\n",
    "I, Finally            Finish\n",
    "\n",
    "Finally, Finish       BT5153\n",
    "</pre>\n",
    "\n",
    "The first input x is required for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 3  # it means three words in and one word out\n",
    "sequences = []\n",
    "for i in range(1, len(encoded)):\n",
    "    # here, we scan the corpus and we will get four grams\n",
    "    sequence = encoded[max(0,i-win_len):i+1]\n",
    "    sequences.append(sequence) \n",
    "# the sequences will contain a list of four grams, i.e., four words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 4\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "max_length = win_len + 1\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each four grams in the sequences, the first three words will be the model input and the last word will be the model output.\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  25],\n",
       "       [  0,  25,   5],\n",
       "       [ 25,   5,   7],\n",
       "       [  5,   7,   9],\n",
       "       [  7,   9,  10],\n",
       "       [  9,  10,  46],\n",
       "       [ 10,  46,  47],\n",
       "       [ 46,  47,  48],\n",
       "       [ 47,  48,  49],\n",
       "       [ 48,  49,  11],\n",
       "       [ 49,  11,  50],\n",
       "       [ 11,  50,  12],\n",
       "       [ 50,  12,  26],\n",
       "       [ 12,  26,  13],\n",
       "       [ 26,  13,   2],\n",
       "       [ 13,   2,  11],\n",
       "       [  2,  11,  51],\n",
       "       [ 11,  51,  52],\n",
       "       [ 51,  52,  53],\n",
       "       [ 52,  53,  54],\n",
       "       [ 53,  54,  17],\n",
       "       [ 54,  17,  27],\n",
       "       [ 17,  27,   2],\n",
       "       [ 27,   2,  11],\n",
       "       [  2,  11,   1],\n",
       "       [ 11,   1,  55],\n",
       "       [  1,  55,  56],\n",
       "       [ 55,  56,  57],\n",
       "       [ 56,  57,  58],\n",
       "       [ 57,  58,  12],\n",
       "       [ 58,  12,  28],\n",
       "       [ 12,  28,  59],\n",
       "       [ 28,  59,   8],\n",
       "       [ 59,   8,  60],\n",
       "       [  8,  60,  61],\n",
       "       [ 60,  61,  62],\n",
       "       [ 61,  62,  63],\n",
       "       [ 62,  63,  29],\n",
       "       [ 63,  29,   2],\n",
       "       [ 29,   2,  64],\n",
       "       [  2,  64,  14],\n",
       "       [ 64,  14,  65],\n",
       "       [ 14,  65,  18],\n",
       "       [ 65,  18,  66],\n",
       "       [ 18,  66,  67],\n",
       "       [ 66,  67,  25],\n",
       "       [ 67,  25,   9],\n",
       "       [ 25,   9,   3],\n",
       "       [  9,   3,   1],\n",
       "       [  3,   1,  68],\n",
       "       [  1,  68,  10],\n",
       "       [ 68,  10,   4],\n",
       "       [ 10,   4,  69],\n",
       "       [  4,  69,  30],\n",
       "       [ 69,  30,  70],\n",
       "       [ 30,  70,  31],\n",
       "       [ 70,  31,  71],\n",
       "       [ 31,  71,  72],\n",
       "       [ 71,  72,  32],\n",
       "       [ 72,  32,   3],\n",
       "       [ 32,   3,   4],\n",
       "       [  3,   4,  73],\n",
       "       [  4,  73,  74],\n",
       "       [ 73,  74,  75],\n",
       "       [ 74,  75,  18],\n",
       "       [ 75,  18,  76],\n",
       "       [ 18,  76,  77],\n",
       "       [ 76,  77,  33],\n",
       "       [ 77,  33,  78],\n",
       "       [ 33,  78,  32],\n",
       "       [ 78,  32,  79],\n",
       "       [ 32,  79,  34],\n",
       "       [ 79,  34,   4],\n",
       "       [ 34,   4,  17],\n",
       "       [  4,  17,  80],\n",
       "       [ 17,  80,  81],\n",
       "       [ 80,  81,   7],\n",
       "       [ 81,   7,   9],\n",
       "       [  7,   9,   3],\n",
       "       [  9,   3,  82],\n",
       "       [  3,  82,   5],\n",
       "       [ 82,   5,  83],\n",
       "       [  5,  83,   5],\n",
       "       [ 83,   5,   6],\n",
       "       [  5,   6,  84],\n",
       "       [  6,  84,  85],\n",
       "       [ 84,  85,   1],\n",
       "       [ 85,   1,  86],\n",
       "       [  1,  86,  87],\n",
       "       [ 86,  87,  10],\n",
       "       [ 87,  10,  33],\n",
       "       [ 10,  33,  31],\n",
       "       [ 33,  31,  88],\n",
       "       [ 31,  88,   8],\n",
       "       [ 88,   8,  17],\n",
       "       [  8,  17,  89],\n",
       "       [ 17,  89,  19],\n",
       "       [ 89,  19,  35],\n",
       "       [ 19,  35,  90],\n",
       "       [ 35,  90,  91],\n",
       "       [ 90,  91,  27],\n",
       "       [ 91,  27,  10],\n",
       "       [ 27,  10,  20],\n",
       "       [ 10,  20,  92],\n",
       "       [ 20,  92,  93],\n",
       "       [ 92,  93,  94],\n",
       "       [ 93,  94,  95],\n",
       "       [ 94,  95,  96],\n",
       "       [ 95,  96,  97],\n",
       "       [ 96,  97,  98],\n",
       "       [ 97,  98,   1],\n",
       "       [ 98,   1,  36],\n",
       "       [  1,  36,   1],\n",
       "       [ 36,   1,  15],\n",
       "       [  1,  15,   6],\n",
       "       [ 15,   6,   4],\n",
       "       [  6,   4,  37],\n",
       "       [  4,  37,  38],\n",
       "       [ 37,  38,  30],\n",
       "       [ 38,  30,  39],\n",
       "       [ 30,  39,   5],\n",
       "       [ 39,   5,   8],\n",
       "       [  5,   8,  40],\n",
       "       [  8,  40,  99],\n",
       "       [ 40,  99, 100],\n",
       "       [ 99, 100,   3],\n",
       "       [100,   3, 101],\n",
       "       [  3, 101, 102],\n",
       "       [101, 102,  41],\n",
       "       [102,  41, 103],\n",
       "       [ 41, 103,   1],\n",
       "       [103,   1,  15],\n",
       "       [  1,  15,   6],\n",
       "       [ 15,   6, 104],\n",
       "       [  6, 104,   2],\n",
       "       [104,   2, 105],\n",
       "       [  2, 105,  21],\n",
       "       [105,  21,   2],\n",
       "       [ 21,   2, 106],\n",
       "       [  2, 106,   6],\n",
       "       [106,   6,   4],\n",
       "       [  6,   4, 107],\n",
       "       [  4, 107,   5],\n",
       "       [107,   5,  40],\n",
       "       [  5,  40, 108],\n",
       "       [ 40, 108, 109],\n",
       "       [108, 109,   3],\n",
       "       [109,   3,  13],\n",
       "       [  3,  13, 110],\n",
       "       [ 13, 110,  42],\n",
       "       [110,  42, 111],\n",
       "       [ 42, 111,  22],\n",
       "       [111,  22,   2],\n",
       "       [ 22,   2,  14],\n",
       "       [  2,  14,  43],\n",
       "       [ 14,  43,   2],\n",
       "       [ 43,   2, 112],\n",
       "       [  2, 112, 113],\n",
       "       [112, 113,  22],\n",
       "       [113,  22,  44],\n",
       "       [ 22,  44, 114],\n",
       "       [ 44, 114, 115],\n",
       "       [114, 115, 116],\n",
       "       [115, 116, 117],\n",
       "       [116, 117,   1],\n",
       "       [117,   1,  16],\n",
       "       [  1,  16,   7],\n",
       "       [ 16,   7, 118],\n",
       "       [  7, 118,   3],\n",
       "       [118,   3,   7],\n",
       "       [  3,   7, 119],\n",
       "       [  7, 119,  23],\n",
       "       [119,  23,  21],\n",
       "       [ 23,  21,   2],\n",
       "       [ 21,   2, 120],\n",
       "       [  2, 120, 121],\n",
       "       [120, 121,  24],\n",
       "       [121,  24, 122],\n",
       "       [ 24, 122, 123],\n",
       "       [122, 123,   8],\n",
       "       [123,   8, 124],\n",
       "       [  8, 124,   7],\n",
       "       [124,   7,   9],\n",
       "       [  7,   9, 125],\n",
       "       [  9, 125,  35],\n",
       "       [125,  35,  14],\n",
       "       [ 35,  14,  34],\n",
       "       [ 14,  34,   4],\n",
       "       [ 34,   4,  23],\n",
       "       [  4,  23,  29],\n",
       "       [ 23,  29,  20],\n",
       "       [ 29,  20,  23],\n",
       "       [ 20,  23,   5],\n",
       "       [ 23,   5,  20],\n",
       "       [  5,  20,  45],\n",
       "       [ 20,  45,  24],\n",
       "       [ 45,  24, 126],\n",
       "       [ 24, 126, 127],\n",
       "       [126, 127,  11],\n",
       "       [127,  11,  19],\n",
       "       [ 11,  19, 128],\n",
       "       [ 19, 128,  19],\n",
       "       [128,  19,  22],\n",
       "       [ 19,  22,   3],\n",
       "       [ 22,   3, 129],\n",
       "       [  3, 129,  12],\n",
       "       [129,  12,  28],\n",
       "       [ 12,  28,   1],\n",
       "       [ 28,   1,  15],\n",
       "       [  1,  15, 130],\n",
       "       [ 15, 130,  12],\n",
       "       [130,  12,  43],\n",
       "       [ 12,  43, 131],\n",
       "       [ 43, 131,   1],\n",
       "       [131,   1,  36],\n",
       "       [  1,  36,  42],\n",
       "       [ 36,  42,  26],\n",
       "       [ 42,  26,  44],\n",
       "       [ 26,  44,   1],\n",
       "       [ 44,   1,  16],\n",
       "       [  1,  16, 132],\n",
       "       [ 16, 132,   8],\n",
       "       [132,   8,   1],\n",
       "       [  8,   1, 133],\n",
       "       [  1, 133,   1],\n",
       "       [133,   1,  15],\n",
       "       [  1,  15, 134],\n",
       "       [ 15, 134,  13],\n",
       "       [134,  13,   1],\n",
       "       [ 13,   1,  16],\n",
       "       [  1,  16,   6],\n",
       "       [ 16,   6,   4],\n",
       "       [  6,   4,  37],\n",
       "       [  4,  37,  38],\n",
       "       [ 37,  38, 135],\n",
       "       [ 38, 135,  21],\n",
       "       [135,  21,   2],\n",
       "       [ 21,   2,   6],\n",
       "       [  2,   6, 136],\n",
       "       [  6, 136, 137],\n",
       "       [136, 137, 138],\n",
       "       [137, 138, 139],\n",
       "       [138, 139, 140],\n",
       "       [139, 140,  41],\n",
       "       [140,  41,   3],\n",
       "       [ 41,   3, 141],\n",
       "       [  3, 141,  45],\n",
       "       [141,  45, 142],\n",
       "       [ 45, 142,  24],\n",
       "       [142,  24, 143],\n",
       "       [ 24, 143,   1],\n",
       "       [143,   1,  16],\n",
       "       [  1,  16, 144],\n",
       "       [ 16, 144,   2],\n",
       "       [144,   2,  14],\n",
       "       [  2,  14, 145],\n",
       "       [ 14, 145,  39],\n",
       "       [145,  39, 146],\n",
       "       [ 39, 146,  18],\n",
       "       [146,  18,   4],\n",
       "       [ 18,   4, 147],\n",
       "       [  4, 147, 148]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3, 15)             2235      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                13200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 149)               7599      \n",
      "=================================================================\n",
      "Total params: 23,034\n",
      "Trainable params: 23,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 15\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=max_length-1)) \n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 1s - loss: 5.0038 - acc: 0.0115\n",
      "Epoch 2/200\n",
      " - 0s - loss: 4.9984 - acc: 0.0496\n",
      "Epoch 3/200\n",
      " - 0s - loss: 4.9934 - acc: 0.0534\n",
      "Epoch 4/200\n",
      " - 0s - loss: 4.9876 - acc: 0.0534\n",
      "Epoch 5/200\n",
      " - 0s - loss: 4.9803 - acc: 0.0534\n",
      "Epoch 6/200\n",
      " - 0s - loss: 4.9701 - acc: 0.0534\n",
      "Epoch 7/200\n",
      " - 0s - loss: 4.9562 - acc: 0.0534\n",
      "Epoch 8/200\n",
      " - 0s - loss: 4.9352 - acc: 0.0534\n",
      "Epoch 9/200\n",
      " - 0s - loss: 4.9017 - acc: 0.0534\n",
      "Epoch 10/200\n",
      " - 0s - loss: 4.8486 - acc: 0.0534\n",
      "Epoch 11/200\n",
      " - 0s - loss: 4.7691 - acc: 0.0534\n",
      "Epoch 12/200\n",
      " - 0s - loss: 4.6776 - acc: 0.0534\n",
      "Epoch 13/200\n",
      " - 0s - loss: 4.6113 - acc: 0.0534\n",
      "Epoch 14/200\n",
      " - 0s - loss: 4.5716 - acc: 0.0534\n",
      "Epoch 15/200\n",
      " - 0s - loss: 4.5306 - acc: 0.0534\n",
      "Epoch 16/200\n",
      " - 0s - loss: 4.4878 - acc: 0.0534\n",
      "Epoch 17/200\n",
      " - 0s - loss: 4.4468 - acc: 0.0573\n",
      "Epoch 18/200\n",
      " - 0s - loss: 4.4023 - acc: 0.0573\n",
      "Epoch 19/200\n",
      " - 0s - loss: 4.3560 - acc: 0.0573\n",
      "Epoch 20/200\n",
      " - 0s - loss: 4.3163 - acc: 0.0573\n",
      "Epoch 21/200\n",
      " - 0s - loss: 4.2701 - acc: 0.0534\n",
      "Epoch 22/200\n",
      " - 0s - loss: 4.2265 - acc: 0.0611\n",
      "Epoch 23/200\n",
      " - 0s - loss: 4.1821 - acc: 0.0687\n",
      "Epoch 24/200\n",
      " - 0s - loss: 4.1386 - acc: 0.0687\n",
      "Epoch 25/200\n",
      " - 0s - loss: 4.0973 - acc: 0.0763\n",
      "Epoch 26/200\n",
      " - 0s - loss: 4.0546 - acc: 0.0802\n",
      "Epoch 27/200\n",
      " - 0s - loss: 4.0134 - acc: 0.0802\n",
      "Epoch 28/200\n",
      " - 0s - loss: 3.9734 - acc: 0.0878\n",
      "Epoch 29/200\n",
      " - 0s - loss: 3.9314 - acc: 0.0954\n",
      "Epoch 30/200\n",
      " - 0s - loss: 3.8940 - acc: 0.1145\n",
      "Epoch 31/200\n",
      " - 0s - loss: 3.8602 - acc: 0.1145\n",
      "Epoch 32/200\n",
      " - 0s - loss: 3.8279 - acc: 0.1336\n",
      "Epoch 33/200\n",
      " - 0s - loss: 3.7946 - acc: 0.1336\n",
      "Epoch 34/200\n",
      " - 0s - loss: 3.7627 - acc: 0.1221\n",
      "Epoch 35/200\n",
      " - 0s - loss: 3.7278 - acc: 0.1298\n",
      "Epoch 36/200\n",
      " - 0s - loss: 3.6979 - acc: 0.1298\n",
      "Epoch 37/200\n",
      " - 0s - loss: 3.6650 - acc: 0.1374\n",
      "Epoch 38/200\n",
      " - 0s - loss: 3.6348 - acc: 0.1374\n",
      "Epoch 39/200\n",
      " - 0s - loss: 3.6037 - acc: 0.1298\n",
      "Epoch 40/200\n",
      " - 0s - loss: 3.5724 - acc: 0.1374\n",
      "Epoch 41/200\n",
      " - 0s - loss: 3.5406 - acc: 0.1641\n",
      "Epoch 42/200\n",
      " - 0s - loss: 3.5055 - acc: 0.1679\n",
      "Epoch 43/200\n",
      " - 0s - loss: 3.4679 - acc: 0.1756\n",
      "Epoch 44/200\n",
      " - 0s - loss: 3.4307 - acc: 0.1756\n",
      "Epoch 45/200\n",
      " - 0s - loss: 3.3912 - acc: 0.1679\n",
      "Epoch 46/200\n",
      " - 0s - loss: 3.3506 - acc: 0.1908\n",
      "Epoch 47/200\n",
      " - 0s - loss: 3.3133 - acc: 0.1985\n",
      "Epoch 48/200\n",
      " - 0s - loss: 3.2739 - acc: 0.2252\n",
      "Epoch 49/200\n",
      " - 0s - loss: 3.2296 - acc: 0.2290\n",
      "Epoch 50/200\n",
      " - 0s - loss: 3.1877 - acc: 0.2328\n",
      "Epoch 51/200\n",
      " - 0s - loss: 3.1444 - acc: 0.2328\n",
      "Epoch 52/200\n",
      " - 0s - loss: 3.0997 - acc: 0.2405\n",
      "Epoch 53/200\n",
      " - 0s - loss: 3.0555 - acc: 0.2443\n",
      "Epoch 54/200\n",
      " - 0s - loss: 3.0101 - acc: 0.2481\n",
      "Epoch 55/200\n",
      " - 0s - loss: 2.9673 - acc: 0.2481\n",
      "Epoch 56/200\n",
      " - 0s - loss: 2.9214 - acc: 0.2557\n",
      "Epoch 57/200\n",
      " - 0s - loss: 2.8727 - acc: 0.2672\n",
      "Epoch 58/200\n",
      " - 0s - loss: 2.8274 - acc: 0.2786\n",
      "Epoch 59/200\n",
      " - 0s - loss: 2.7838 - acc: 0.2824\n",
      "Epoch 60/200\n",
      " - 0s - loss: 2.7360 - acc: 0.2939\n",
      "Epoch 61/200\n",
      " - 0s - loss: 2.6921 - acc: 0.3244\n",
      "Epoch 62/200\n",
      " - 0s - loss: 2.6482 - acc: 0.3321\n",
      "Epoch 63/200\n",
      " - 0s - loss: 2.6011 - acc: 0.3359\n",
      "Epoch 64/200\n",
      " - 0s - loss: 2.5573 - acc: 0.3435\n",
      "Epoch 65/200\n",
      " - 0s - loss: 2.5116 - acc: 0.3626\n",
      "Epoch 66/200\n",
      " - 0s - loss: 2.4709 - acc: 0.3626\n",
      "Epoch 67/200\n",
      " - 0s - loss: 2.4232 - acc: 0.3855\n",
      "Epoch 68/200\n",
      " - 0s - loss: 2.3818 - acc: 0.4008\n",
      "Epoch 69/200\n",
      " - 0s - loss: 2.3358 - acc: 0.4275\n",
      "Epoch 70/200\n",
      " - 0s - loss: 2.2902 - acc: 0.4466\n",
      "Epoch 71/200\n",
      " - 0s - loss: 2.2475 - acc: 0.4580\n",
      "Epoch 72/200\n",
      " - 0s - loss: 2.2018 - acc: 0.4695\n",
      "Epoch 73/200\n",
      " - 0s - loss: 2.1633 - acc: 0.4962\n",
      "Epoch 74/200\n",
      " - 0s - loss: 2.1247 - acc: 0.5076\n",
      "Epoch 75/200\n",
      " - 0s - loss: 2.0823 - acc: 0.5153\n",
      "Epoch 76/200\n",
      " - 0s - loss: 2.0442 - acc: 0.5267\n",
      "Epoch 77/200\n",
      " - 0s - loss: 2.0063 - acc: 0.5420\n",
      "Epoch 78/200\n",
      " - 0s - loss: 1.9692 - acc: 0.5611\n",
      "Epoch 79/200\n",
      " - 0s - loss: 1.9313 - acc: 0.5649\n",
      "Epoch 80/200\n",
      " - 0s - loss: 1.8941 - acc: 0.5916\n",
      "Epoch 81/200\n",
      " - 0s - loss: 1.8568 - acc: 0.6069\n",
      "Epoch 82/200\n",
      " - 0s - loss: 1.8244 - acc: 0.5916\n",
      "Epoch 83/200\n",
      " - 0s - loss: 1.7884 - acc: 0.6183\n",
      "Epoch 84/200\n",
      " - 0s - loss: 1.7587 - acc: 0.6298\n",
      "Epoch 85/200\n",
      " - 0s - loss: 1.7270 - acc: 0.6679\n",
      "Epoch 86/200\n",
      " - 0s - loss: 1.6942 - acc: 0.6565\n",
      "Epoch 87/200\n",
      " - 0s - loss: 1.6649 - acc: 0.6603\n",
      "Epoch 88/200\n",
      " - 0s - loss: 1.6293 - acc: 0.6756\n",
      "Epoch 89/200\n",
      " - 0s - loss: 1.5947 - acc: 0.6908\n",
      "Epoch 90/200\n",
      " - 0s - loss: 1.5649 - acc: 0.7061\n",
      "Epoch 91/200\n",
      " - 0s - loss: 1.5344 - acc: 0.7176\n",
      "Epoch 92/200\n",
      " - 0s - loss: 1.5094 - acc: 0.7252\n",
      "Epoch 93/200\n",
      " - 0s - loss: 1.4769 - acc: 0.7366\n",
      "Epoch 94/200\n",
      " - 0s - loss: 1.4501 - acc: 0.7557\n",
      "Epoch 95/200\n",
      " - 0s - loss: 1.4244 - acc: 0.7557\n",
      "Epoch 96/200\n",
      " - 0s - loss: 1.3983 - acc: 0.7519\n",
      "Epoch 97/200\n",
      " - 0s - loss: 1.3732 - acc: 0.7634\n",
      "Epoch 98/200\n",
      " - 0s - loss: 1.3480 - acc: 0.7557\n",
      "Epoch 99/200\n",
      " - 0s - loss: 1.3208 - acc: 0.7672\n",
      "Epoch 100/200\n",
      " - 0s - loss: 1.2980 - acc: 0.7748\n",
      "Epoch 101/200\n",
      " - 0s - loss: 1.2737 - acc: 0.7748\n",
      "Epoch 102/200\n",
      " - 0s - loss: 1.2470 - acc: 0.7901\n",
      "Epoch 103/200\n",
      " - 0s - loss: 1.2277 - acc: 0.7901\n",
      "Epoch 104/200\n",
      " - 0s - loss: 1.2020 - acc: 0.8015\n",
      "Epoch 105/200\n",
      " - 0s - loss: 1.1796 - acc: 0.8130\n",
      "Epoch 106/200\n",
      " - 0s - loss: 1.1581 - acc: 0.8321\n",
      "Epoch 107/200\n",
      " - 0s - loss: 1.1378 - acc: 0.8282\n",
      "Epoch 108/200\n",
      " - 0s - loss: 1.1169 - acc: 0.8359\n",
      "Epoch 109/200\n",
      " - 0s - loss: 1.0963 - acc: 0.8435\n",
      "Epoch 110/200\n",
      " - 0s - loss: 1.0740 - acc: 0.8511\n",
      "Epoch 111/200\n",
      " - 0s - loss: 1.0548 - acc: 0.8435\n",
      "Epoch 112/200\n",
      " - 0s - loss: 1.0333 - acc: 0.8550\n",
      "Epoch 113/200\n",
      " - 0s - loss: 1.0171 - acc: 0.8511\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.9994 - acc: 0.8588\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.9826 - acc: 0.8740\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.9644 - acc: 0.8664\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.9464 - acc: 0.8779\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.9285 - acc: 0.8817\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.9117 - acc: 0.8779\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.8944 - acc: 0.8969\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.8795 - acc: 0.8931\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.8642 - acc: 0.8969\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.8499 - acc: 0.9046\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.8336 - acc: 0.9160\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.8195 - acc: 0.9237\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.8040 - acc: 0.9313\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.7891 - acc: 0.9237\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.7772 - acc: 0.9351\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.7635 - acc: 0.9313\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.7490 - acc: 0.9351\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.7381 - acc: 0.9351\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.7248 - acc: 0.9427\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.7114 - acc: 0.9427\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.6989 - acc: 0.9466\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.6876 - acc: 0.9580\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.6762 - acc: 0.9466\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.6666 - acc: 0.9466\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.6546 - acc: 0.9504\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.6424 - acc: 0.9542\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.6319 - acc: 0.9542\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.6231 - acc: 0.9504\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.6112 - acc: 0.9542\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.6003 - acc: 0.9580\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.5932 - acc: 0.9580\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.5845 - acc: 0.9618\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.5722 - acc: 0.9618\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.5617 - acc: 0.9695\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.5547 - acc: 0.9656\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.5443 - acc: 0.9656\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.5351 - acc: 0.9695\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.5255 - acc: 0.9695\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.5171 - acc: 0.9695\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.5092 - acc: 0.9695\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.5030 - acc: 0.9656\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.4944 - acc: 0.9656\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.4859 - acc: 0.9656\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.4770 - acc: 0.9656\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.4708 - acc: 0.9656\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.4630 - acc: 0.9618\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.4539 - acc: 0.9695\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.4469 - acc: 0.9733\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.4408 - acc: 0.9733\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.4348 - acc: 0.9771\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.4270 - acc: 0.9771\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.4195 - acc: 0.9771\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.4132 - acc: 0.9771\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.4076 - acc: 0.9733\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.4003 - acc: 0.9771\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.3953 - acc: 0.9771\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.3892 - acc: 0.9771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      " - 0s - loss: 0.3852 - acc: 0.9771\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.3774 - acc: 0.9771\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.3739 - acc: 0.9733\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.3673 - acc: 0.9771\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.3608 - acc: 0.9809\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.3558 - acc: 0.9771\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.3507 - acc: 0.9733\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.3455 - acc: 0.9771\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.3402 - acc: 0.9771\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.3367 - acc: 0.9771\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.3314 - acc: 0.9771\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.3256 - acc: 0.9809\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.3229 - acc: 0.9885\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.3178 - acc: 0.9885\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.3128 - acc: 0.9847\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.3084 - acc: 0.9924\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.3035 - acc: 0.9885\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.2992 - acc: 0.9885\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.2953 - acc: 0.9847\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.2911 - acc: 0.9885\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.2870 - acc: 0.9885\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.2837 - acc: 0.9885\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.2798 - acc: 0.9885\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.2765 - acc: 0.9847\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.2724 - acc: 0.9885\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.2690 - acc: 0.9885\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.2656 - acc: 0.9847\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.2621 - acc: 0.9885\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.2582 - acc: 0.9885\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.2553 - acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2eccfca3be0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### call model to generate text for n-gram language model\n",
    "\n",
    "- Here, we can use our trained model to write our own `Harry Potter`\n",
    "- The idea is that we can iteratively call the language model to select the word with the highest prob scores. \n",
    "    - Inital the first word as w0\n",
    "    - Loop index i from 0 to the pre-defined length n_words\n",
    "        1. feed the word wi-n+1, wi-n+2,...,wi into the model\n",
    "        2. assign the word with the highest probs. score to wi+1\n",
    "        3. index i = i + 1\n",
    "    - At last, given word w0, we have the complete sentence w0, w1,..., wn_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    \"\"\"\n",
    "    Model inputs:\n",
    "    1 model: language model\n",
    "    2 tokenizer: it maintain the same mapping (word to index) as the model used\n",
    "    3 max_length: it is for padding if the length of input word lists is less than the required one\n",
    "    3 seed_text: the inital input word string\n",
    "    4 n_words: the length of target sentence\n",
    "    \"\"\"\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write our own `Harry Potter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potters arrived in the street the dursleys knew\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, tokenizer, max_length-1, 'Potters arrived in', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variable-gram Language Model\n",
    "\n",
    "- Variable Number word in, one word out\n",
    "- The assumption is that the current word depends on all its previous words\n",
    "- P('I finally finish BT5153') = P(I) * P(finally/I) * P(finish/I, finally) * P(BT5153/I, finally,finish)\n",
    "- Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stateful RNN\n",
    "We can do this in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.\n",
    "\n",
    "This is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).\n",
    "\n",
    "We first need to define our LSTM layer as stateful. In so doing, we must explicitly specify the batch size as a dimension on the input shape. This also means that when we evaluate the network or make predictions, we must also specify and adhere to this same batch size. This is not a problem now as we are using a batch size of 1. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.\n",
    "\n",
    "some useful tutorials for stateful RNN:\n",
    "1. http://philipperemy.github.io/keras-stateful-lstm/\n",
    "2. https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n",
    "3. https://keras.io/getting-started/faq/#how-can-i-use-stateful-rnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data generation\n",
    "\n",
    "The training data is the same as the first one-word in and one-word out mode. \n",
    "\n",
    "For stateful RNN, the idea is to split the sequence into elements of size 1 and feed them to the LSTM. Once the sequence is over, we manually reset the states of the RNN to have a clean setup for the next one. For each element, we associate the related target Yi. Because the RNN is stateful, the state will be propagated to the next batch. Also because the batch_size=1, we are sure that the state of the last element will be used as input to the current element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 262\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model = Sequential()\n",
    "embedding_size = 15\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=1, batch_input_shape=(batch_size,1)))\n",
    "model.add(LSTM(50, batch_input_shape=(batch_size, 1, 10), stateful=True))   ## The stateful is true\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1s - loss: 5.0134 - acc: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.8542 - acc: 0.0382\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.6247 - acc: 0.0534\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5889 - acc: 0.0611\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5496 - acc: 0.0649\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4451 - acc: 0.0534\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.2653 - acc: 0.0649\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0665 - acc: 0.0840\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.1282 - acc: 0.0534\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.8721 - acc: 0.0916\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7705 - acc: 0.1107\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.6054 - acc: 0.1450\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5230 - acc: 0.1565\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2780 - acc: 0.1756\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3419 - acc: 0.1565\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0631 - acc: 0.2099\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9996 - acc: 0.2061\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0025 - acc: 0.1985\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9994 - acc: 0.2405\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7835 - acc: 0.3321\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7471 - acc: 0.2672\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5538 - acc: 0.3473\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4768 - acc: 0.3893\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6227 - acc: 0.3206\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4466 - acc: 0.3969\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5921 - acc: 0.3168\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4816 - acc: 0.3473\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3193 - acc: 0.4198\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1239 - acc: 0.5153\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0840 - acc: 0.5076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9576 - acc: 0.5229\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1107 - acc: 0.4618\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9686 - acc: 0.5267\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8041 - acc: 0.5916\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8273 - acc: 0.5802\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6485 - acc: 0.6450\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0565 - acc: 0.4809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5745 - acc: 0.6527\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6208 - acc: 0.6489\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0103 - acc: 0.5458\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8194 - acc: 0.6145\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3852 - acc: 0.7252\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4244 - acc: 0.6985\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6857 - acc: 0.6107\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3607 - acc: 0.7099\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1601 - acc: 0.7977\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4347 - acc: 0.7061\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2018 - acc: 0.7824\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1104 - acc: 0.8053\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1697 - acc: 0.7786\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3227 - acc: 0.6870\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1879 - acc: 0.7405\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0480 - acc: 0.8206\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0440 - acc: 0.7901\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8460 - acc: 0.8893\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8267 - acc: 0.8893\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8523 - acc: 0.8626\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7954 - acc: 0.8893\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7845 - acc: 0.8931\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6940 - acc: 0.9008\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7263 - acc: 0.9008\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8200 - acc: 0.8588\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5994 - acc: 0.9313\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6365 - acc: 0.9084\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0319 - acc: 0.7634\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6789 - acc: 0.9160\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4886 - acc: 0.9656\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4119 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3959 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3959 - acc: 0.9656\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3737 - acc: 0.9542\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4109 - acc: 0.9580\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3552 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4137 - acc: 0.9504\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4048 - acc: 0.9313\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3090 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5368 - acc: 0.8969\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6331 - acc: 0.8626\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3874 - acc: 0.9580\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6127 - acc: 0.8664\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3204 - acc: 0.9695\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2969 - acc: 0.9618\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2753 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3404 - acc: 0.9542\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2318 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2272 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1849 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2105 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1821 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1876 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1886 - acc: 0.9733\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2092 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3478 - acc: 0.9389\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3742 - acc: 0.9313\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2853 - acc: 0.9427\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6956 - acc: 0.8053\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4299 - acc: 0.9122\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4855 - acc: 0.9046\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2224 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1599 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1515 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1478 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1243 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1378 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1040 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2402 - acc: 0.9504\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1342 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1180 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1243 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1206 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1082 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1328 - acc: 0.9695\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1168 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0795 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0726 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0800 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1099 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1335 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1300 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0947 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1852 - acc: 0.9466\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1474 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2418 - acc: 0.9466\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2016 - acc: 0.9504\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2308 - acc: 0.9542\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1415 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1264 - acc: 0.9695\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1485 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0848 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0770 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0633 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0689 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0604 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0580 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0566 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0560 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0662 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0594 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0584 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0471 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0573 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0781 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0635 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0490 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0472 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.0526 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.1379 - acc: 0.9618\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1679 - acc: 0.9580\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0846 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1106 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2065 - acc: 0.9466\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1539 - acc: 0.9656\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0765 - acc: 0.9924\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0739 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0480 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0367 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0339 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0333 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0324 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0354 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0291 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0259 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0267 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0250 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0244 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0272 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0351 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0478 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0719 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0450 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0353 - acc: 0.9885\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0999 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1041 - acc: 0.9733\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0914 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0793 - acc: 0.9847\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0795 - acc: 0.9771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0812 - acc: 0.9733\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0796 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2376 - acc: 0.9275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0755 - acc: 0.9733\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0408 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0311 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0312 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0233 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0159 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0163 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1383 - acc: 0.9618\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1329 - acc: 0.9580\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1075 - acc: 0.9695\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0616 - acc: 0.9809\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0291 - acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0293 - acc: 0.9924\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for i in range(nb_epoch):\n",
    "    model.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()   ## it is important for stateful RNN, after each scanning one document, you should call reset_states()\n",
    "                           ## here, we only have one document so that we call reset_states per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write our own `Harry Potter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "director they were dursley of were as\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq_one_word(model, tokenizer, 'director', 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
