{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 CNN for text classifcation on Keras\n",
    "In this notebook, we are going to implement three language models following three language models following their own assumptions\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Data Preprocessing\n",
    "\n",
    "2. CNN for text classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "- Here, we load the IMDB review corpus.\n",
    "- We preprocess the corpus and are going to use two columns including sentiment (label) and review text (input data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "train = pd.read_csv(\"../BT5153_data/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    \n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = review_text.lower().split()                             \n",
    "    #\n",
    "    return( \" \".join(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"with all this stuff going down at the moment with mj i\\'ve started listening to his music, watching the odd documentary here and there, watched the wiz and watched moonwalker again. maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. some of it has subtle messages about mj\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring. some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him.the actual feature film bit when it finally starts is only on for 20 minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord. why he wants mj dead so bad is beyond me. because mj overheard his plans? nah, joe pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates mj\\'s music.lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence. also, the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.bottom line, this movie is for people who like mj on one level or another (which i think is most people). if not, then stay away. it does try and give off a wholesome message and ironically mj\\'s bestest buddy in this movie is a girl! michael jackson is truly one of the most talented people ever to grace this planet but is he guilty? well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. he is either an extremely nice but stupid guy or one of the most sickest liars. i hope he is not the latter.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the review sentence\n",
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN\n",
    "\n",
    "Here, CNN is used for sentiment analysis. Given a review, positive or negative labels are inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For demo purpose, the first 1000 samples are used as training data\n",
    "## In addition, 100 samples are used as testing/validation set\n",
    "train_reviews = clean_train_reviews[:1000]\n",
    "test_reviews = clean_train_reviews[1000:1100]\n",
    "all_labels = train[\"sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = all_labels[:1000]\n",
    "test_labels = all_labels[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =  8000\n",
    "tk = Tokenizer(num_words=vocab_size)  ## here, we are set the max number of words to keep. The most common 7999 words will be kept\n",
    "tk.fit_on_texts(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_reviews)\n",
    "test_texts = tk.texts_to_sequences(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since reviews have different lengths, we need to make all the reviews having the same length  \n",
    "- The unified length is set to be the max length of all documents in training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = max([len(ele) for ele in train_sequences]) \n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=sequence_length, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')\n",
    "train_classes = np.array(train_labels, dtype='int')\n",
    "test_classes = np.array(test_labels, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Model API\n",
    "Keras provides a Model class that you can use to create a model from your created layers. It requires that you only specify the input and output layers.\n",
    "\n",
    "https://machinelearningmastery.com/keras-functional-api-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   CNN Framework\n",
    "\n",
    "1. This is a CNN network for sentence classification.\n",
    "\n",
    "2. Filters Sizes are 2, 3, 4.\n",
    "\n",
    "3. In our following implementation, filters sizes are 2,3,4. Each filter size has 30 filters. The embeddings size is 20. \n",
    "\n",
    "<img src=\"cnn.png\" alt=\"cnn\"\n",
    "\ttitle=\"cnn pic\" width=\"600\" height=\"200\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20    # The size of embeddings is 20\n",
    "input_shape = (sequence_length,)\n",
    "model_input = Input(shape=input_shape)\n",
    "# Embedding Layer\n",
    "z = Embedding(vocab_size, embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "# Convolutional Layer \n",
    "conv_blocks = []\n",
    "filter_sizes = [2,3,4]\n",
    "num_filters = 30\n",
    "for sz in filter_sizes:\n",
    "    # sz is the window size\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=sz,\n",
    "                  padding=\"valid\",\n",
    "                  activation=\"relu\",\n",
    "                  strides=1)(z)\n",
    "    # Pooling Layer\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    # if you call MaxPooling1D(), you need use flatten to remove the axis 2\n",
    "    conv_blocks.append(conv)\n",
    "# Fully-connected Layer\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "# It is binary classifcation problem. We can use sigmoid layer.\n",
    "# If it is multi-class classifcaiton problem, we can use softmax layer \n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Keras, MaxPooling1D() vs GlobalMaxPooling1D()\n",
    "\n",
    "https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.6931 - acc: 0.4990 - val_loss: 0.6889 - val_acc: 0.5400\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.6757 - acc: 0.5910 - val_loss: 0.6862 - val_acc: 0.5400\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.6625 - acc: 0.6060 - val_loss: 0.6817 - val_acc: 0.5400\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.6475 - acc: 0.8640 - val_loss: 0.6765 - val_acc: 0.6700\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.6282 - acc: 0.9740 - val_loss: 0.6650 - val_acc: 0.6700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a112a897b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(train_data, train_classes,\n",
    "          validation_data=(test_data, test_classes),\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
