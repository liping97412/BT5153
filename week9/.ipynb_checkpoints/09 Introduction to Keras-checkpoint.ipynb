{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/keras-logo-small.jpg\" width=\"20%\" />\n",
    "\n",
    "## Keras: The Python Deep Learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Reading in the Kaggle data, prepare BoW features as inputs and one-hot vector as labels\n",
    "2. Using Keras for Neural Network\n",
    "3. Add more layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow, CNTK or Theano. \n",
    "\n",
    "* It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "ref: https://keras.io/\n",
    "\n",
    "* Tensorflow is a deep learning lib for numerical computation and machine intelligence. As a open source resource, data flow graphs are adopted for numerical computation. Mathematical operations are represented by nodes and tensors are represented by graph edges. It is sometimes extremely technical.\n",
    "\n",
    "* In contrast, Keras makes deep neural network coding simple. It also runs seamlessly on CPU and GPU machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Kaggle data, prepare BoW features\n",
    "\n",
    "- Our goal is to predict the **cuisine** of a recipe, given its **ingredients**.\n",
    "- **Feature engineering** is the process through which you create features that don't natively exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts a DataFrame and adds new features\n",
    "def make_features(df):\n",
    "    # string representation of the ingredient list\n",
    "    df['ingredients_str'] = df.ingredients.astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same features in the training data and the new data\n",
    "train = make_features(pd.read_json('C:/Users/Administrator/Desktop/MSBA/BT5153 TOPICS IN BUSINESS ANALYTICS/BT5153_data/week6_train.json'))\n",
    "new = make_features(pd.read_json('C:/Users/Administrator/Desktop/MSBA/BT5153 TOPICS IN BUSINESS ANALYTICS/BT5153_data/week6_test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the regex pattern that is used for tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(token_pattern=r\"'([a-z ]+)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = train.ingredients_str\n",
    "y = train.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['romaine lettuce', 'black olives', 'grape tom...\n",
       "1    ['plain flour', 'ground pepper', 'salt', 'toma...\n",
       "2    ['eggs', 'pepper', 'salt', 'mayonaise', 'cooki...\n",
       "3          ['water', 'vegetable oil', 'wheat', 'salt']\n",
       "4    ['black pepper', 'shallots', 'cornflour', 'cay...\n",
       "Name: ingredients_str, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is just a Series of strings\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# to avoid the following warning: `DeprecationWarning: The truth value of an empty array is ambiguous`,\n",
    "# update your sklearn version above 0.20.0\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding\n",
    "\n",
    "   1. LabelEncoder: encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier. \n",
    "\n",
    "`For categorical variables where no such ordinal relationship exists, the integer encoding learned from LabelEncoder is not enough.`\n",
    "   2. OnehotEncoder: convert the distinct interger into vector which is filled with 0 and 1.\n",
    "   \n",
    "`In neural network for classification, the last layer is always softmax layer, which require one-hot vector`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#interger encoding is not good enough, cause we give some of the attributes more weight\n",
    "# one-hot encoding, y is a list of string\n",
    "le = LabelEncoder()\n",
    "le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino',\n",
       "       'french', 'greek', 'indian', 'irish', 'italian', 'jamaican',\n",
       "       'japanese', 'korean', 'mexican', 'moroccan', 'russian',\n",
       "       'southern_us', 'spanish', 'thai', 'vietnamese'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from string to its unique number\n",
    "def encode(y_str):\n",
    "    # from string to its unique number\n",
    "    y_numeric = le.transform(y_str)\n",
    "    # from unique number to one-hot vector\n",
    "    y_onehot = to_categorical(y_numeric)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform\n",
    "def decode(vec_onehot):\n",
    "    idx = np.argmax(vec_onehot)\n",
    "    return le.inverse_transform([idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "italian\n"
     ]
    }
   ],
   "source": [
    "print(y_onehot[10])\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Standard Deep Learning Pipeline in Keras\n",
    "\n",
    "1. Model Construction (two ways)\n",
    "    * Sequential \n",
    "    * Funcation API\n",
    "2. Model Compiling\n",
    "    * loss function\n",
    "    * optimization functions\n",
    "3. Model Training\n",
    "4. Model Predictions\n",
    "5. Model save and re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_dtm))\n",
    "X_train = X_dtm.todense()\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse vector format:\n",
      "  (0, 2033)\t1\n",
      "  (0, 2361)\t1\n",
      "  (0, 4943)\t1\n",
      "  (0, 4450)\t1\n",
      "  (0, 4100)\t1\n",
      "  (0, 2367)\t1\n",
      "  (0, 2511)\t1\n",
      "  (0, 464)\t1\n",
      "  (0, 4755)\t1\n",
      "dense vector format:\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('sparse vector format:')\n",
    "print(X_dtm[0])\n",
    "print('dense vector format:')\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250 dims\n",
      "Building model...\n",
      "20 classes\n"
     ]
    }
   ],
   "source": [
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')\n",
    "print(\"Building model...\")\n",
    "\n",
    "Y_train = y_onehot\n",
    "nb_classes = Y_train.shape[1]\n",
    "print(nb_classes, 'classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model in the Sequential Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# add layers to model\n",
    "model.add(Dense(400, input_shape=(dims,), activation='relu')) #only first layers \n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "    1. loss and optimizer are two reuqired arguments for compiling a keras model\n",
    "    2. different optimizer may result in various performances, try 'sgd' and check the performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Activation** Supported : [https://keras.io/activations/] \n",
    "Advanced: [https://keras.io/layers/advanced-activations/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Optimizer**\n",
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n",
    "Here we used <b>adam</b> (Adaptive Moment Estimation) as an optimization algorithm for our trainable weights.  \n",
    "\n",
    "<img src=\"http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif\" width=\"40%\">\n",
    "\n",
    "Source & Reference: http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of optimization is quite important, you may try 'sgd' to replace 'adam' and compare the performances \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network fit function, several key terms are batch_size, epoch and iterations:\n",
    "\n",
    "1. one epoch: one forward pass and one backward pass of all the training examples\n",
    "2. batch size: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "3. number of iterations: number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "39774/39774 [==============================] - 12s 292us/step - loss: 2.1183 - acc: 0.5117\n",
      "Epoch 2/5\n",
      "39774/39774 [==============================] - 9s 237us/step - loss: 1.0801 - acc: 0.7281\n",
      "Epoch 3/5\n",
      "39774/39774 [==============================] - 9s 216us/step - loss: 0.7566 - acc: 0.8000\n",
      "Epoch 4/5\n",
      "39774/39774 [==============================] - 8s 211us/step - loss: 0.6153 - acc: 0.8306\n",
      "Epoch 5/5\n",
      "39774/39774 [==============================] - 9s 235us/step - loss: 0.5320 - acc: 0.8520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124a0c8d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=1024, epochs=5)\n",
    "#the model will train very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_strtest = new.ingredients_str\n",
    "X_dtmtest = vect.transform(X_strtest)\n",
    "X_test    = X_dtmtest.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.36937577e-05 1.25375800e-05 4.15923190e-04 2.24857722e-05\n",
      " 2.43239101e-05 1.69380556e-03 4.42192395e-04 1.47851842e-05\n",
      " 1.63494533e-05 9.94349658e-01 8.11868267e-06 1.00494626e-05\n",
      " 1.80442366e-05 1.01241772e-03 3.20408508e-05 1.41383325e-05\n",
      " 1.40296924e-03 2.85323535e-04 1.17317082e-04 1.39473213e-05]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "italian\n"
     ]
    }
   ],
   "source": [
    "# check the predicted results\n",
    "Y_testpred = model.predict(X_test)\n",
    "\n",
    "print(Y_testpred[10])\n",
    "print(Y_train[10])\n",
    "print(decode(Y_testpred[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "italian\n"
     ]
    }
   ],
   "source": [
    "# another way to check the probability\n",
    "Y_testlabel = model.predict_classes(X_test)\n",
    "\n",
    "# check the predicted results\n",
    "print(Y_testlabel[10])\n",
    "print(le.inverse_transform([Y_testlabel[10]])[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_label = [decode(vec) for vec in Y_testpred]\n",
    "# create a submission file\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new_pred_label}).set_index('id').to_csv('sub5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model save and re-load\n",
    "\n",
    ">The model has a `save` method, which saves all the details necessary to reconstitue the model. You can check the following example:\n",
    "\n",
    "https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "model.predict_classes(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: How to add more layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplicity is pretty impressive right? :) Keras is based on object-oriented design principles. Its author `François Chollet` said\n",
    "<pre>Another important decision was to use an <b>object-oriented design</b>. Deep learning models can be understood as <b>chains</b> of functions, thus making a functional approach look potentially interesting. However, these functions are heavily parameterized, mostly by their weight tensors, and manipulating these parameters in a functional way would just be impractical. So in Keras, <b>everything is an object</b>: layers, models, optimizers, etc. All parameters of a model can be accessed as object properties: e.g. `model.layers[3].output` is the output tensor of the 3rd layer in the model, `model.layers[3].weights` is the list of symbolic weight tensors of the layer, and so on.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets understand:\n",
    "<pre>The core data structure of Keras is a <b>model</b>, a way to organize layers. The main type of model is the <b>Sequential</b> model, a linear stack of layers. Sequential() can return a kind of container which can call the add function to add layers. </pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/MLP.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** _How hard can it be to build a Multi-Layer Fully-Connected Network with keras?_\n",
    "\n",
    "**A:** _It is basically the same, just add more layers!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 400)               2500400   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 600)               240600    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                12020     \n",
      "=================================================================\n",
      "Total params: 2,753,020\n",
      "Trainable params: 2,753,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# add layers to model\n",
    "model.add(Dense(400, input_shape=(dims,), activation='relu')) #only first layers \n",
    "model.add(Dense(600, activation='relu')) #only first layers \n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39774/39774 [==============================] - 14s 348us/step - loss: 1.9461 - acc: 0.4767\n",
      "Epoch 2/5\n",
      "39774/39774 [==============================] - 10s 260us/step - loss: 0.8590 - acc: 0.7513\n",
      "Epoch 3/5\n",
      "39774/39774 [==============================] - 10s 255us/step - loss: 0.5736 - acc: 0.8338\n",
      "Epoch 4/5\n",
      "39774/39774 [==============================] - 10s 259us/step - loss: 0.4436 - acc: 0.8691\n",
      "Epoch 5/5\n",
      "39774/39774 [==============================] - 11s 282us/step - loss: 0.3595 - acc: 0.8939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab7527048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=1024, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Motivations for depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Much has been studied about the depth of neural nets. Is has been proven mathematically[1] and empirically that convolutional neural network benifit from depth! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] - On the Expressive Power of Deep Learning: A Tensor Analysis - Cohen, et al 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Motivations for depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One much quoted theorem about neural network states that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] - Approximation Capabilities of Multilayer Feedforward Networks - Kurt Hornik 1991"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
